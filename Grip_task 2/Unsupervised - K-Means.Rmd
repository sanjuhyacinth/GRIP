---
title: "GRIP Task 2 - Unsupervised Algorithm"
author: "Sanju Hyacinth C"
date: "14/11/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

# PROJECT AIM: 
To find the optimum number of clusters from the dataset and visualise them

# DATA:
Iris - Open source dataset

# ALGORITHM:
K-Means - Unsupervised Algorithm


```{r, include=FALSE}

setwd("C:/Users/DELL/Desktop/Sanju/GRIP/Grip_task 2")
getwd()

```

# INSTALLING REQUIRED LIBRARIES:

```{r}

# install.packages("patchwork")
# install.packages("tidyverse")
# install.packages("gridExtra")
# install.packages("ggExtra")
# install.packages("gtable")
# install.packages("ggpubr")

```


# EXPLORATORY DATA ANALYSIS:

```{r}

# loading data
dsba2 = read.csv("dsba_2.csv",header = TRUE)

# top 10 rows
head(dsba2, 10)

# shape of the data
dim(dsba2)

# structure of the data
str(dsba2)

```

```{r}

# summary of the data:
summary(dsba2)

```

# Inference:

* No NA values are present in the data.
* We have 50 numbers each of the species - Setosa, Versicolor & Virginica
* The mean and median are not far apart indicating less number of outliers.

# DATA VISUALIZATION:

## Boxplots - to examine outliers:

```{r}

library(gridExtra)
library(ggplot2)
library(ggpubr)


# SEPAL LENGTH:

plot1 = ggplot(data = dsba2)+ geom_boxplot(aes(x = dsba2$Species, 
                                               y = dsba2$SepalLengthCm, 
                                               fill = dsba2$Species), width=0.5) + 
  labs(title = "Boxplot - Sepal Length", 
       x = "Species", y = "Sepal Length") + theme(legend.position = "none")

# SEPAL WIDTH:

plot2 = ggplot(data = dsba2)+ geom_boxplot(aes(x = dsba2$Species, 
                                               y = dsba2$SepalWidthCm, 
                                               fill = dsba2$Species), width=0.5) + 
  labs(title = "Boxplot - Sepal Width", x = "Species", 
       y = "Sepal Width") + theme(legend.position = "none")

# PETAL LENGTH:

plot3 = ggplot(data = dsba2)+ geom_boxplot(aes(x = dsba2$Species, 
                                               y = dsba2$PetalLengthCm, 
                                               fill = dsba2$Species), width=0.5) + 
  labs(title = "Boxplot - Petal Length", x = "Species", 
       y = "Petal Length") + theme(legend.position = "none")

# PETAL WIDTH:

plot4 = ggplot(data = dsba2)+ geom_boxplot(aes(x = dsba2$Species, 
                                               y = dsba2$PetalWidthCm, 
                                               fill = dsba2$Species), width=0.5) + 
  labs(title = "Boxplot - Petal Width", x = "Species", 
       y = "Petal Width") + theme(legend.position = "none")


grid.arrange(plot1, plot2, plot3, plot4, ncol=2)

```

# Inference:

* We have some outliers in all the columns.
* Virginica and Setosa are respectively the largest and the smallest of the flowers.
* Sepal Width is different from the other attributes.


```{r}

# Correlation:

library(corrplot)

dsba2_plot = corrplot(cor(dsba2[, 2:5]), 
                      method = "number", 
                      tl.cex = 0.5, 
                      tl.col = "dark blue")

```

#Finding the optimal number of clusters:

```{r}

## Finding optimal number of clusters from WSS

wssplot = function(data, nc=15, seed=123){
  wss = (nrow(data)-1)*sum(apply(data,2,var))
  for (i in 2:nc){
    set.seed(seed)
    wss[i] <- sum(kmeans(data, centers=i)$withinss)}
  plot(1:nc, wss, type="b", xlab="Number of Clusters",
       ylab="Within groups sum of squares")}

wssplot(dsba2[,2:5], nc=10)

```

```{r}

## Identifying the optimal number of clusters using NbClust:

library(NbClust)

set.seed(123)
Nclus <- NbClust(dsba2[,2:5], min.nc=2, max.nc=5, method="kmeans")
table(Nclus$Best.n[1,])

barplot(table(Nclus$Best.n[1,]),
          xlab="Numer of Clusters", ylab="Number of Criteria",
          main="Number of Clusters Chosen by 26 Criteria")

# According to the majority rule, the best number of clusters is  2

```

# Forming & Plotting the clusters:

```{r}

kmeans_clust = kmeans(x=dsba2[,2:5], centers = 2, nstart = 5)
kmeans_clust

# K-means clustering with 2 clusters of sizes 97, 53
# the percentage similarity between data in the same cluster is 77.6 


library(fpc)
library(cluster)

## plotting the clusters

plotcluster(dsba2[,2:5], kmeans_clust$cluster)

# More complex

clusplot(dsba2[,2:5], main = "Clusterplot - When k=2", 
         kmeans_clust$cluster, 
         color=TRUE, shade=TRUE, labels=2, lines=1)

```

# Let us look for the similarity percentage with 3 clusters:

```{r}

# Forming & Plotting the clusters:

kmeans_clust2 = kmeans(x=dsba2[,2:5], centers = 3, nstart = 5)
kmeans_clust2

# K-means clustering with 3 clusters of sizes 38, 50, 62
# the percentage similarity between data in the same cluster is 88.4 %
# The increase of one cluster can increase similarity about 11% which is great.

## plotting the clusters

plotcluster(dsba2[,2:5], kmeans_clust2$cluster)

# More complex

clusplot(dsba2[,2:5], main = "Clusterplot - When k=3", 
         kmeans_clust2$cluster, 
         color=TRUE, shade=TRUE, labels=2, lines=1)

```

# Clustering Validation:

We may use the silhouette coefficient (silhouette width) to evaluate the goodness of our clustering. We will see the clusters silhouette plot and measure the average silhouette width to confirm which is better. The more the value is closer to 1, the better is the clustering of data points. Hence, the model (k value) that has the best silhouette coefficient will be confirmed.

```{r}

library(factoextra)

# When k=2
sil_k2 <- silhouette(kmeans_clust$cluster, dist(dsba2[,2:5]))
fviz_silhouette(sil_k2)

# Cluster 1 (97 data points) has avg.sil.width of 0.63
# Cluster 2 (53 data points) has avg.sil.width of 0.77
# Average silhouette width : 0.68

# When k=3
sil_k3 <- silhouette(kmeans_clust2$cluster, dist(dsba2[,2:5]))
fviz_silhouette(sil_k3)

# Cluster 1 (38 data points) has avg.sil.width of 0.45
# Cluster 2 (50 data points) has avg.sil.width of 0.80
# Cluster 3 (62 data points) has avg.sil.width of 0.42
# Average silhouette width : 0.55

```

# Inference:

Upon observing the clusters, we find that the average silhouette width when k=2 is more than when k=3. And since the individual silhouette width of both the clusters in model 1 is more than 0.5 (closer to 1) than the 3 clusters from the next model, we choose k=2 as the optimum number of clusters.



















