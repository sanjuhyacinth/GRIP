{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRIP Task #1 - Prediction of Student Percentage\n",
    "\n",
    "## Dataset Outline\n",
    "We are given a student data, with two attributes for this analysis - number of hours studied as the independent variable and percentage as the dependent variable. We are asked to predict the percentage of students by analysis the patterns with the number of hours. We have to, in this case find out the **predicted score or the percentage, if a student studies for 9.25 hours per day**. \n",
    "\n",
    "## Model used\n",
    "We are using a **Supervisied Machine Learning Algorithm - Linear Regression**, for this prediction since the dependent variable is numeric in nature. And since we have just one independent variable, it is a **Simple Linear Regression model**. We will use the **scikit-learn** library in Jupyter notebook for this prediction analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load the _pyforest_ library just so we know we have all the needed Data Science libraries are always present in our work environment without having to remember any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pyforest\n",
    "# from pyforest import *\n",
    "\n",
    "lazy_imports()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional libraries we need for analysis:\n",
    "\n",
    "from sklearn.model_selection import train_test_split   # Subsetting data\n",
    "from sklearn.linear_model import LinearRegression      # Model building\n",
    "from sklearn import metrics                            # Model evaluation\n",
    "from sklearn.metrics import r2_score                   # model efficiency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us look into the dataset we are provided with, the description of the independent and dependent (target) variables for analysis\n",
    "\n",
    "## <center>Data Description</center>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><b><center>Variable</center></b></td>\n",
    "        <td><b><center>Definition</center></b></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><center>Hours</center></td>\n",
    "        <td>Number of hours the student studies (Independent variable - X)<td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><center>Scores</center></td>\n",
    "        <td>The percentage of the student (Target variable - y)<td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "         \n",
    "## TABLE OF CONTENTS:\n",
    "\n",
    "* **A. Exploratory Data Analysis**\n",
    "     1. Data Summary (EDD) & Inference\n",
    "     2. Data Visualization: \n",
    "        _a. Distance plot & Inference_\n",
    "        _b. Scatter plot & Inference_\n",
    "        \n",
    "        \n",
    "* **B. Data Modelling**\n",
    "     1. Data Splitting\n",
    "     2. Model Creation & Interpretation\n",
    "     3. Model Prediction\n",
    "     4. Model Evaluation\n",
    "     5. Score Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Exploratory Data Analysis\n",
    "\n",
    "Let us now get going with the student data loading and analysis. In this section, we will see a summary of the data, visualize them for insights and then consolidate our inferences from the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Summary (EDD)\n",
    "\n",
    "We will see the data loading and the data summary in this part of exploratory analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsba1 = pd.read_csv(\"dsba_1 - Sheet1.csv\")\n",
    "print(\"Dataset loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The top five entries of the Student Data: \")\n",
    "dsba1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The bottom five entries of the Student Data: \")\n",
    "dsba1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of the dataset: \", dsba1.shape)\n",
    "\n",
    "print(\"The dataset has 2 attributes and 25 entries in each attribute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names of columns\n",
    "\n",
    "print(\"Column names: \") \n",
    "dsba1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsba1.info()\n",
    "\n",
    "# The datatypes are float and integer for hours and scores respectively. \n",
    "# We will have to convert the datatype of scores to float since the predicted score can bear decimals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting tagret variable to float:\n",
    "\n",
    "dsba1.Scores = dsba1.Scores.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsba1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summary of the dataset:\")\n",
    "\n",
    "dsba1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference:\n",
    "\n",
    "From the summary of the attributes, we can confirm that\n",
    "\n",
    "* There are no missing values present in the dataset since the count is 25 for both attributes\n",
    "* We can say that the data does not carry many outliers because the mean and median (50%) values are not far apart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Visualization \n",
    "\n",
    "In this part of the analysis, we will try to visualise the dataset using plots and graphs. We shall firstly do distribution charts and then a sctter plot to establish the relationship between the dependent and independent variables\n",
    "\n",
    "### a. Distance plot:\n",
    "\n",
    "Since being numeric data, let us first examine the distribution (histogram) of both the study hours and scores of the students separately to see the spread of the data. This will give us a picture of the spread or skewness of the variable, if there is any or help us ascertain if it is a normal distribution. We will make use of the **distplot** function from the **seaborn library** to do this. The bins are set at 20, to better understand the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='whitegrid', palette=\"pastel\", font_scale=1.1, rc={\"figure.figsize\": [7, 6]})\n",
    "sns.distplot(dsba1['Hours'], norm_hist=False, bins=20 )\n",
    "plt.title('Distribution of Study Hours', fontsize = 15)\n",
    "plt.xlabel('Hours of study')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='whitegrid', palette=\"pastel\", font_scale=1.1, rc={\"figure.figsize\": [7, 6]})\n",
    "sns.distplot(dsba1['Scores'], norm_hist=False, bins=20, color='pink')\n",
    "plt.title('Percentage Distribution', fontsize = 20)\n",
    "plt.xlabel('Student Scores')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference:\n",
    "\n",
    "* There is no significant skewness found in the graphs\n",
    "* The highest peaks are at 2.5 hrs and close to 30% score\n",
    "* The density curves suggest that the study hours seem more consistent that the scores which seem to show a clear dip, may state that students either score low or high but not in between.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Scatter Plot:\n",
    "\n",
    "We are now using the scatter plot to see the extent of relationship between the two variables, since we have already found out some similarities between them. We are also going to fit a **regression line** along with the scatter plot just to see the fit.\n",
    "For this we use the **lmplot** that fits a best-fit linear regression line along with a **95% confidence interval** band that the population regression line will lie within the interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (15, 5)\n",
    "sns.lmplot(x=\"Hours\", y=\"Scores\", data=dsba1)\n",
    "plt.title('Relationship between Hours and Scores', fontsize=14)  \n",
    "plt.xlabel('Hours Studied', fontsize=12)  \n",
    "plt.ylabel('Percentage', fontsize=12)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference:\n",
    "\n",
    "This plot shows that there is a **direct positive relationship** between the two variables, as an increase in the independent variables (Hours) brings a significant increase in the dependent variable (Scores). This scatter plot with the regression line was used to bring in a best-fit line in an attempt to minimise errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Data Modelling\n",
    "\n",
    "We come to the next stage of the analysis, which is the data modeling. This involves the below steps:\n",
    "\n",
    "* Data Splitting\n",
    "* Model Fitting\n",
    "* Model Evaluation\n",
    "\n",
    "\n",
    "## 1. Data Splitting:\n",
    "\n",
    "We start by splitting our dataset into **train and validation data** in order to effectively train and validate the model's performance. For this we use the **train_test_split** function from the scikit-learn library. By this, we can split the data into 4 subsets: training and validation dataset with X and y separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dsba1.iloc[:, :-1].values  \n",
    "y = dsba1.iloc[:, 1:].values  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are assigning our independent variable to X and our dependent variable to y. \n",
    "\n",
    "We are using the **index locator** function - iloc. This helps us assign the attributes using their corresponding index positions. \n",
    "\n",
    "* For X, we are taking all the rows while leaving out the last column. **:-1** means that we are taking columns from 0 through second last column that is represented as -1.\n",
    "* For y, we are simply taking only the last column (1 in this case since we have only 2 attributes) and all the rows of course.\n",
    "\n",
    "Now, we are splitting the data from X and y to training and validation sets, giving us 4 subsets. It is important to set the same **random state** to a specific number for reproduceability. We are taking 0 here and the test split as 0.25% of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA SPLITTING:\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of the split datasets:\n",
    "\n",
    "print(\"Shape of datasets: \")\n",
    "X_train.shape,  X_val.shape,  y_train.shape,  y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing X_train\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now train our Linear Regression Algorithm on the X_train data. Evaluate its results on the X_val and then test the same on our y_train and y_val dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Creation: \n",
    "\n",
    "Let us import the **LinearRegression** function from the scikit-learn library to fit the linear regression model to the dataset. We are fitting the model onto the X_train and y_train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION \n",
    "\n",
    "regr = LinearRegression()  \n",
    "\n",
    "regr.fit(X_train, y_train) \n",
    "print(\"The data training is complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our model, regr is trained, let's have a quick view of the model scores, **coefficient and intercept** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coefficients: \", regr.coef_[0][0])\n",
    "print(\"Intercept: \", regr.intercept_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Interpretation \n",
    "\n",
    "From our model of the form **y = mX + c**,\n",
    "\n",
    "m (coefficient) = 9.9417\n",
    "\n",
    "c (intercept)   = 1.9322  \n",
    "\n",
    "Coefficient m is the effect of X on y. And intercept c is the point where the line passes through y axis when X=0. This is otherwise known as the bias component in the formula that remains a constant for the equation.\n",
    "\n",
    "An increase/decrease in 1 unit of the independent variable (hour), increases/decreases the target variable by **9.9 units**, meaning if a student increases/decreases his/her study hours by 1, his score goes up/down by 9.9% .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Prediction:\n",
    "\n",
    "Let us test the efficiency of the model's prediction on the validation set, X_val. Upon doing so, we will also compare the results with y_val as well. For that we will plot them together to find how close the prediction has been made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting y for the validation set:\n",
    "\n",
    "predict_y = regr.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now taking a look at the **r squared value**, regression score function. This gives us a score between 0 and 1 of how strong the true y value and the predicted ones correlate. For this test, we use the **r2_score** from the **sklearn.metrics** library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2 score:\n",
    "\n",
    "r2_score(y_val, predict_y)\n",
    "\n",
    "# 0.94 seems to be very good "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation:\n",
    "\n",
    "We are now going to evaluate the accuracy of the model by using the below model accuracy metrics\n",
    "\n",
    "* Mean Absolute Error **(MAE)**\n",
    "* Mean Squared Error **(MSE)**\n",
    "* Root Mean Squared Error **(RMSE)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the accuracy using evaluation metrics:\n",
    "\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_val, predict_y))\n",
    "print('Mean Squared Error: ', metrics.mean_squared_error(y_val, predict_y))\n",
    "print('Root Mean Squared Error: ', np.sqrt(metrics.mean_squared_error(y_val, predict_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the Mean Absolute Error and Root Mean Squared Error are almost in line, but we are choosing the MAE which has given the best result.\n",
    "\n",
    "#### Let us compare the model's prediction accuracy with the original scores of the students.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compr = pd.DataFrame({'Actual': y_val.flatten(), 'Predicted': predict_y.flatten()})\n",
    "compr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphical representation\n",
    "\n",
    "compr.plot(kind='bar')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Score Prediction:\n",
    "\n",
    "We are asked to find out the students' score if he/she studies for 9.25 hours a day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrs = [[9.25]]\n",
    "pred_hrs = regr.predict(hrs)\n",
    "print(\"No of Hours: \", hrs)\n",
    "print(\"Predicted Score: \", pred_hrs[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try our hand at predicting the score for 9.5 hours and see the increase in score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing for 9.5 hours of study\n",
    "\n",
    "hrs2 = [[9.5]]\n",
    "pred_hrs2 = regr.predict(hrs2)\n",
    "print(\"No of Hours: \", hrs2)\n",
    "print(\"Predicted Score: \", pred_hrs2[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference of scores with 15 min increase in study:\n",
    "\n",
    "diff = pred_hrs2-pred_hrs\n",
    "\n",
    "print(\"Score increase with 15 minutes more study = \", diff[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this predictive analysis of the student scores data using simple linear regression, we have xplained the following\n",
    "\n",
    "* Visualise the **degree of relationship** between the independent and dependent variable\n",
    "* Find out the **coefficient (m)** of the independent variable and the **intercept (c)** for prediction\n",
    "* Evaluated the model's accuracy using **performance metrics**. The best result was given by **MAE = 4.13**\n",
    "* R squared value is found to be 0.936 \n",
    "* **Predicted** the score of the student with 9.25 study hrs/day\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>THE END</center>\n",
    "\n",
    "<center>Project done by Sanju Hyacinth C</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
